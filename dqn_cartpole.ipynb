{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sradicwebster/RL_implementation/blob/master/dqn_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Cartpole implementation\n",
    "The RL objective is to learn a policy that maximises return - the discounted, cumulative reward:\n",
    "$$ R_{t0} = \\sum_{t=t0}^\\infty \\gamma^{tâˆ’t0} r_t $$\n",
    "\n",
    "If we had a correct Q-function which maps from state-action pair to rewards ($Q^*:State \\space x \\space Action \\rightarrow \\mathbb{R}$) then we would know which action to take in a given state that maximises return by choosing the action which the highest Q-value:\n",
    "$$ \\pi^*(s) = \\underset{a}{\\mathrm{argmin}} \\space Q^*(s,a) $$\n",
    "\n",
    "We go about learning the Q-function from interacting in an environment by forming an update equation based on the  Bellman equation (the Q-value of being state $s$ and taking action $a$ is the immediate reward plus the discounted Q-value of the subsequent state and taking actions following policy $\\pi$):\n",
    "$$ Q_\\pi(s,a) = r + \\gamma Q(s^\\prime,\\pi(s^\\prime))  $$\n",
    "\n",
    "The right hand side of the equation is known as the target $y$ and the difference between the current Q-value estimation $Q(s,a)$ and $y$ is the temporal difference (TD) error which is minimised in the Q-learning algorithm.\n",
    "\n",
    "The Deep Q-Networks (DQN) algorithm combines the Q-Learning algorithm with deep neural networks for approximating the Q-function. DQN uses an experience replay memory which stores the transitions that the agent observes, allowing us to reuse this data later. Batches of transitions are sampled to update the parameters of the Q-function by gradient decent to minimise the TD error (MSE loss or Huber loss). By sampling a batch randomly, the transitions are decorrelated which stabilises and improves the DQN training procedure.\n",
    "\n",
    "Overestimation of Q-values causes large positive biases in updating procedure so in practice we use a different DNN to estimate the target Q-value to stabilise the learning process (giving the algorithm the name double DQN referring to the 2 networks). This target network is fixed and is used to calculate the expected return $ \\hat{Q}(s^\\prime,a^\\prime;\\theta^\\prime) $. The TD error is now:\n",
    "\n",
    "$$ TD = r + \\gamma \\space max_{a^\\prime} \\hat{Q}(s^\\prime,a^\\prime;\\theta^\\prime) - Q(s,a;\\theta) $$\n",
    "\n",
    "The action-value (or policy) neural network $ Q(s,a;\\theta) $ is updated as batches of transitions are sampled. The target network parameters are updated every so often (say 1000 iterations) by taking a copy of the policy network.\n",
    "\n",
    "<img src=\"dqn_algo.png\" style=\"width:504px;height:410px;\">\n",
    "\n",
    "Mnih et al (2015) [Human Level Control Through Deep Reinforcement Learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)\n",
    "\n",
    "#### Dueling DQN\n",
    "Decomposes Q-function into state value (parameterised by $\\alpha$) and advantage (parameterised by $\\beta$) for each action. The value of a state is independent of action. Implementation involves the addition of an aggregation layer in network: $$Q(s, a; \\alpha, \\beta) = V(s; \\alpha) + A(s, a; \\beta) - \\frac{1}{|A|} \\sum_{a^{\\prime}} A(s, a; \\beta) $$\n",
    "\n",
    "<img src=\"dueling.png\" style=\"width:250px;height:210px;\">\n",
    "\n",
    "Wang et al (2015) [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581v3)\n",
    "\n",
    "#### Prioritised Experience Replay (to be implemented)\n",
    "Changes the sampling distribution from uniform (random) to favour experiences that are deemed more important according to the absolute TD error, which indicates how unexpected a certain transition was (plus a small constant e). The probability of transition i being chosen:\n",
    "$$ p_i = \\frac{(TD_i + e)^a}{\\sum_k (TD_k + e)^a} $$\n",
    "where a is hyperparameter: a=1 is uniform, a=0 is greedy\n",
    "\n",
    "The Q-value estimation with stochastic updates requires that the updates correspond to the same distribution as the expectation. However, we have introduced a bias toward sampling high-priority experiences so risk overfitting. To correct this bias, we use importance sampling (IS) weights (which reduces the TD error for high priority experiences):\n",
    "$$ w_i = \\left(\\frac{1}{N}\\frac{1}{p_i}\\right)^\\beta $$\n",
    "where $\\beta$ is a hyperparameter. Normally $\\beta$ starts from zero and gradually reaches 1 ($\\beta=1$ then non-uniform probabilities are fully compensated).\n",
    "\n",
    "Schaul et al (2016) [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)\n",
    "\n",
    "Use [bisect](https://docs.python.org/3/library/bisect.html) for implementation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "id": "hU6_Hef4RXFD",
    "outputId": "481de456-d3ee-4afa-bb31-935027b022a9"
   },
   "outputs": [],
   "source": [
    "# Uncomment for colab\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "Xzarg1-RQOX1",
    "outputId": "97327a40-8900-4b1c-cc4a-0678eed8fd25"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='dqn_cartpole')\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU', torch.cuda.is_available())\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.wrappers.Monitor(env, '.dqn_video/', video_callable=lambda episode_id: episode_id%100==0, force=True)\n",
    "\n",
    "# Get size of observation space\n",
    "obs_size = env.observation_space.shape[0]\n",
    "print(f'Observation space: {obs_size}')\n",
    "# Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity \n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "print(f'Action space: {n_actions}')\n",
    "# Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3hqezzPkQOYB",
    "outputId": "b87b8950-f48c-4fa2-e9f6-a0404e67a41c"
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'terminal'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def store(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self): # this needed??\n",
    "        return len(self.memory)\n",
    "\n",
    "    \n",
    "class Qnetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Dueling_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dueling_network, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(obs_size, 64)\n",
    "        self.fc_value = nn.Linear(64, 128)\n",
    "        self.fc_adv = nn.Linear(64, 128)\n",
    "\n",
    "        self.value = nn.Linear(128, 1)\n",
    "        self.adv = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        value = F.relu(self.fc_value(x))\n",
    "        adv = F.relu(self.fc_adv(x))\n",
    "\n",
    "        value = self.value(value)\n",
    "        adv = self.adv(adv)\n",
    "\n",
    "        advAverage = torch.mean(adv, dim=0, keepdim=True)\n",
    "        Q = value + adv - advAverage\n",
    "\n",
    "        return Q\n",
    "\n",
    "    \n",
    "def forward_prop(network, state):\n",
    "    return network(torch.from_numpy(state).float().to(device))\n",
    "\n",
    "def epsilon_greedy_action(state):\n",
    "    \n",
    "    epsilon = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * episode / EPS_DECAY)\n",
    "    wandb.log({\"epsilon\": epsilon}, step=episode)\n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        return torch.tensor(random.randrange(n_actions)).to(device)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(forward_prop(Q_net, state)).to(device)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    minibatch = memory.sample(BATCH_SIZE)\n",
    "    targets = []\n",
    "    current_qs = []\n",
    "    for transition in minibatch:\n",
    "\n",
    "        if transition.terminal is True:\n",
    "            target = torch.tensor(transition.reward, device=device)\n",
    "        else:\n",
    "            target = transition.reward + GAMMA * forward_prop(target_net, transition.next_state).max()\n",
    "\n",
    "        current_q = forward_prop(Q_net, transition.state)[transition.action]\n",
    "\n",
    "        targets.append(target)\n",
    "        current_qs.append(current_q)\n",
    "\n",
    "    loss = loss_fn(torch.stack(current_qs).to(device), torch.stack(targets).to(device))\n",
    "    wandb.log({\"loss\": loss}, step=episode)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in Q_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "9MqPRE-EQOYR",
    "outputId": "60b16651-15a1-4d4a-8eb1-45c5120b7d2b"
   },
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 5000\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "UPDATE_TARGET_FREQ = 1000 # steps between target network updates (C parameter)\n",
    "# epsilon \n",
    "EPS_START = 1\n",
    "EPS_END = 0.0\n",
    "EPS_DECAY = 250\n",
    "NETWORK = Qnetwork # Dueling_network\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "# Save model inputs and hyperparameters\n",
    "wandb.config = wandb.config\n",
    "wandb.config.learning_rate = LEARNING_RATE\n",
    "wandb.config.batch_size = BATCH_SIZE\n",
    "wandb.config.update_target = UPDATE_TARGET_FREQ\n",
    "wandb.config.epsilon_start = EPS_START\n",
    "wandb.config.epsilon_end = EPS_END\n",
    "wandb.config.epsilon_decay = EPS_DECAY\n",
    "wandb.config.memoery_size = MEMORY_SIZE\n",
    "\n",
    "\n",
    "# initialise parameterised action-value functions\n",
    "Q_net = NETWORK().to(device) # Q net gets updated\n",
    "target_net = NETWORK().to(device) # target net updated set to equal Q net every UPDATE_TARGET steps\n",
    "target_net.load_state_dict(Q_net.state_dict())\n",
    "wandb.config.network = Q_net.__class__.__name__\n",
    "\n",
    "nodes = []\n",
    "params = list(Q_net.parameters())\n",
    "for i in range(len(params))[1::2]:\n",
    "    nodes.append(params[i].size()[0])\n",
    "wandb.config.nn_nodes = nodes\n",
    "\n",
    "optimizer = optim.Adam(Q_net.parameters(), lr=LEARNING_RATE)\n",
    "memory = ReplayMemory(MEMORY_SIZE) # comment out for offline\n",
    "loss_fn = torch.nn.MSELoss() #SmoothL1Loss()# Huber loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "9MqPRE-EQOYR",
    "outputId": "60b16651-15a1-4d4a-8eb1-45c5120b7d2b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episode_rewards = []\n",
    "steps_done = 0\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    \n",
    "    # reset step count\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # get start state from env\n",
    "    state = env.reset() \n",
    "    \n",
    "    # timers\n",
    "    time = []\n",
    "    \n",
    "    terminal = False\n",
    "    while terminal is False:\n",
    "        \n",
    "        # choose next action\n",
    "        action = epsilon_greedy_action(state)\n",
    "        \n",
    "        # take next step and get reward from env\n",
    "        next_state, reward, terminal, _ = env.step(action.item())\n",
    "        \n",
    "        # store in memory\n",
    "        memory.store(state, action, next_state, reward, terminal) # comment out for offline\n",
    "        \n",
    "        # updates\n",
    "        state = next_state\n",
    "        steps_done += 1\n",
    "        episode_reward += reward\n",
    "       \n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        \n",
    "        if terminal:\n",
    "            episode_rewards.append(episode_reward)\n",
    "            wandb.log({\"reward\": episode_reward})\n",
    "            break\n",
    "            \n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if steps_done % UPDATE_TARGET_FREQ == 0:\n",
    "            target_net.load_state_dict(Q_net.state_dict())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.ave_reward = np.mean(episode_rewards[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "dqn_cartpole.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
