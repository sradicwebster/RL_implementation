{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "PPO is based on the theory from trust region policy optimization (TRPO) which constraints the policy improvement step during each iteration. The policy improvement objective function in TRPO is:\n",
    "$$ \\theta_{k+1} = \\underset{a}{\\mathrm{argmax}}\\ \\mathcal{L}(\\theta_k,\\theta) \\ s.t. \\ \\tilde{D}_{KL}(\\theta_k||\\theta) \\leq \\delta $$\n",
    "where\n",
    "$$ \\mathcal{L}(\\theta_k,\\theta) = \\underset{s,a \\sim \\pi_{\\theta_k}}{E} \\left[ \\frac{\\pi_\\theta(a|s)}\n",
    "{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}(a|s)}(s,a) \\right] $$\n",
    "$$ \\tilde{D}_{KL}(\\theta_k||\\theta) = \\underset{s \\sim \\pi_{\\theta_k}}{E} [D_{KL}(\\pi_theta(\\cdot|s)||\\pi_{\\theta_k}(\\cdot|s))] $$\n",
    "The loss function contains a surrogate advantage, a measure of how the new policy performs relative to the old policy using data from the old policy. The constraint limits the average KL divergence between policies across states visited by the old policy. The implementation uses Taylor expansion approximation, Lagrangian duality and a backtracking line search solved using the conjugate gradient algorithm.\n",
    "\n",
    "Like TRPO, PPO constraints policy improvement but in a simpler way and empirically has been shown to work just as well. There are two primary variants of PPO:\n",
    "- PPO-Penalty: penalises the KL-divergence in the objective function with automatic adjustment of the penalty coefficient \n",
    "- PPO-Clip: clipping the objective function to remove incentives for the new policy to get far from the old policy (no KL constraint/penalty)\n",
    "\n",
    "PPO-clip has been implemented here using the following loss for the policy improvement step:\n",
    "$$ \\mathcal{L}(\\theta_k,\\theta) = min \\left( \\frac{\\pi_\\theta(a|s)}\n",
    "{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}(a|s)}(s,a),\\ g(\\epsilon, A^{\\pi_{\\theta_k}(a|s)}(s,a)) \\right) $$\n",
    "where\n",
    "$$ g(\\epsilon,\\ A) = {\\begin{cases}(1+\\epsilon)A \\ \\text{if} \\ A \\geq 0 \\\\(1-\\epsilon)A\\ \\text{if}\\ A < 0 \\end{cases}}  $$\n",
    "Clipping acts as a regulariser controlled by the hyperparameter $\\epsilon$.\n",
    "\n",
    "The PPO algorithm collects a set of trajectories from interaction with the environment and then performs a series of constrained policy improvement steps. The constraint limits the improvement of the new policy to maximum ratio of $\\epsilon$ relative to the policy that collected the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='ppo_cartpole')\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU', torch.cuda.is_available())\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Get size of observation space\n",
    "obs_size = env.observation_space.shape[0]\n",
    "print(f'Observation space: {obs_size}')\n",
    "# Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity \n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "print(f'Action space: {n_actions}')\n",
    "# Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_Vfunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Vfunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def actor(state):\n",
    "    policy = policy_net(torch.from_numpy(state).float().to(device))\n",
    "    probs = F.softmax(policy, dim=0)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    action = dist.sample().item()\n",
    "    return action, probs[action]\n",
    "\n",
    "def rewards_to_go(rewards):\n",
    "    rewards = torch.Tensor(rewards).to(device)\n",
    "    returns = torch.stack([rewards[i:].sum() for i in range(len(rewards))]).to(device)\n",
    "    return returns\n",
    "\n",
    "def get_value(states):\n",
    "    return torch.stack([v_net(torch.from_numpy(state).float().to(device))[0] for state in states]).to(device)\n",
    "\n",
    "def optimise_v_net(returns, values):\n",
    "    loss_v_net = loss_fn(returns, values)\n",
    "    wandb.log({\"value_loss\": loss_v_net}, step=episode)\n",
    "    optimizer_v_net.zero_grad()\n",
    "    loss_v_net.to(device)\n",
    "    loss_v_net.backward()\n",
    "    optimizer_v_net.step()\n",
    "\n",
    "def get_lob_prob(state, action):\n",
    "    policy = policy_net(torch.from_numpy(state).float().to(device))\n",
    "    log_prob = F.log_softmax(policy, dim=0)[action]    \n",
    "    return log_prob\n",
    "\n",
    "def optimise_policy(loss):\n",
    "    \n",
    "    optimizer_policy.zero_grad()\n",
    "    loss.to(device)\n",
    "    loss.backward()\n",
    "    optimizer_policy.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE_POLICY = 1e-4\n",
    "LEARNING_RATE_VALUE = 5e-4 # want critic to update faster\n",
    "MINIBATCH = 32\n",
    "CLIP_RATIO = 0.5\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "# Save model inputs and hyperparameters\n",
    "wandb.config = wandb.config\n",
    "wandb.config.gamma = GAMMA\n",
    "wandb.config.learning_rate_policy = LEARNING_RATE_POLICY\n",
    "wandb.config.learning_rate_value = LEARNING_RATE_VALUE\n",
    "wandb.config.minibatch = MINIBATCH\n",
    "wandb.config.clipratio = CLIP_RATIO\n",
    "\n",
    "# initialise parameterised policy function\n",
    "policy_net = MLP_policy().to(device) \n",
    "optimizer_policy = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE_POLICY)\n",
    "\n",
    "v_net = MLP_Vfunction().to(device)\n",
    "optimizer_v_net = optim.Adam(v_net.parameters(), lr=LEARNING_RATE_VALUE)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "nodes = []\n",
    "params = list(policy_net.parameters())\n",
    "for i in range(len(params))[1::2]:\n",
    "    nodes.append(params[i].size()[0])\n",
    "wandb.config.policy_nodes = nodes\n",
    "\n",
    "nodes = []\n",
    "params = list(v_net.parameters())\n",
    "for i in range(len(params))[1::2]:\n",
    "    nodes.append(params[i].size()[0])\n",
    "wandb.config.value_nodes = nodes\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    \n",
    "    # tracking\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    old_action_probs = []\n",
    "    \n",
    "    # get start state from env\n",
    "    state = env.reset()\n",
    "    \n",
    "    terminal = False\n",
    "    while terminal is False:\n",
    "        \n",
    "        # choose next action\n",
    "        action, prob = actor(state)\n",
    "        \n",
    "        # take next step and get reward from env\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        # tracking\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        old_action_probs.append(prob.item())\n",
    "        rewards.append(reward*np.power(GAMMA, step)) \n",
    "        \n",
    "        # updates\n",
    "        step += 1\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if step % MINIBATCH == 0 or terminal == True:\n",
    "            \n",
    "            # next state for TD calc\n",
    "            states.append(next_state)\n",
    "            \n",
    "            # reward to go \n",
    "            returns = rewards_to_go(rewards)\n",
    "            \n",
    "            # value estimate\n",
    "            values = get_value(states)\n",
    "        \n",
    "            # advantage estimation using TD error\n",
    "            adv = torch.Tensor([rewards[i] + (1-terminal) * GAMMA * values[i+1].item() - values[i].item() for i in range(len(rewards))])\n",
    "            \n",
    "            # update value net\n",
    "            optimise_v_net(returns, values[:-1])\n",
    "        \n",
    "            for i in range(len(actions)):\n",
    "                policy = policy_net(torch.from_numpy(states[i]).float().to(device))\n",
    "                probs = F.softmax(policy, dim=0)\n",
    "                ratio = probs[actions[i]] / old_action_probs[i]\n",
    "                clip_adv = torch.clamp(ratio, 1-CLIP_RATIO, 1+CLIP_RATIO) * adv[i]\n",
    "                loss_policy = -torch.min(ratio * adv[i], clip_adv)\n",
    "        \n",
    "                # update policy net\n",
    "                optimise_policy(loss_policy)\n",
    "            \n",
    "            \n",
    "            if episode != num_episodes-1 or terminal is False:\n",
    "                # clear lists\n",
    "                states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                old_action_probs = []\n",
    "    \n",
    "\n",
    "    \n",
    "    wandb.log({\"reward\": int(episode_reward)}, step=episode)\n",
    "    episode_rewards.append(int(episode_reward))\n",
    "\n",
    "wandb.config.ave_rewards = np.mean(episode_rewards[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
