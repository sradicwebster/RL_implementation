{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msradicwebster\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">chocolate-fire-43</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sradicwebster/ppo_cartpole\" target=\"_blank\">https://wandb.ai/sradicwebster/ppo_cartpole</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sradicwebster/ppo_cartpole/runs/c41nnq30\" target=\"_blank\">https://wandb.ai/sradicwebster/ppo_cartpole/runs/c41nnq30</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201201_083044-c41nnq30</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU False\n",
      "Observation space: 4\n",
      "Action space: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='ppo_cartpole')\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU', torch.cuda.is_available())\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Get size of observation space\n",
    "obs_size = env.observation_space.shape[0]\n",
    "print(f'Observation space: {obs_size}')\n",
    "# Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity \n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "print(f'Action space: {n_actions}')\n",
    "# Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_Vfunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Vfunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def actor(state):\n",
    "    policy = policy_net(torch.from_numpy(state).float().to(device))\n",
    "    probs = F.softmax(policy, dim=0)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    action = dist.sample().item()\n",
    "    return action, probs[action]\n",
    "\n",
    "def rewards_to_go(rewards):\n",
    "    rewards = torch.Tensor(rewards).to(device)\n",
    "    returns = torch.stack([rewards[i:].sum() for i in range(len(rewards))]).to(device)\n",
    "    return returns\n",
    "\n",
    "def get_value(states):\n",
    "    return torch.stack([v_net(torch.from_numpy(state).float().to(device))[0] for state in states]).to(device)\n",
    "\n",
    "def optimise_v_net(returns, values):\n",
    "    loss_v_net = loss_fn(returns, values)\n",
    "    wandb.log({\"value_loss\": loss_v_net}, step=episode)\n",
    "    optimizer_v_net.zero_grad()\n",
    "    loss_v_net.to(device)\n",
    "    loss_v_net.backward()#retain_graph=True\n",
    "    optimizer_v_net.step()\n",
    "\n",
    "def get_lob_prob(state, action):\n",
    "    policy = policy_net(torch.from_numpy(state).float().to(device))\n",
    "    log_prob = F.log_softmax(policy, dim=0)[action]    \n",
    "    return log_prob\n",
    "\n",
    "def optimise_policy(states, actions, old_action_probs, adv):\n",
    "    \n",
    "    ratio = []\n",
    "    for i in range(len(actions)):\n",
    "        policy = policy_net(torch.from_numpy(states[i]).float().to(device))\n",
    "        probs = F.softmax(policy, dim=0)\n",
    "        ratio.append(probs[actions[i]] / old_action_probs[i])\n",
    "    ratio = torch.stack(ratio).to(device)\n",
    "    \n",
    "    clip_adv = torch.clamp(ratio, 1-CLIP_RATIO, 1+CLIP_RATIO) * adv\n",
    "    loss_policy = -torch.min(ratio * adv, clip_adv).mean()\n",
    "    \n",
    "    optimizer_policy.zero_grad()\n",
    "    loss_policy.to(device)\n",
    "    loss_policy.backward()#retain_graph=True)\n",
    "    optimizer_policy.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [05:50<00:00,  5.71it/s]\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE_POLICY = 1e-4\n",
    "LEARNING_RATE_VALUE = 5e-4 # want critic to update faster\n",
    "MINIBATCH = 32\n",
    "CLIP_RATIO = 0.1\n",
    "\n",
    "num_episodes = 2000\n",
    "\n",
    "# Save model inputs and hyperparameters\n",
    "wandb.config = wandb.config\n",
    "wandb.config.gamma = GAMMA\n",
    "wandb.config.learning_rate_policy = LEARNING_RATE_POLICY\n",
    "wandb.config.learning_rate_value = LEARNING_RATE_VALUE\n",
    "wandb.config.minibatch = MINIBATCH\n",
    "wandb.config.clipratio = CLIP_RATIO\n",
    "\n",
    "# initialise parameterised policy function\n",
    "policy_net = MLP_policy().to(device) \n",
    "optimizer_policy = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE_POLICY)\n",
    "\n",
    "v_net = MLP_Vfunction().to(device)\n",
    "optimizer_v_net = optim.Adam(v_net.parameters(), lr=LEARNING_RATE_VALUE)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "nodes = []\n",
    "params = list(policy_net.parameters())\n",
    "for i in range(len(params))[1::2]:\n",
    "    nodes.append(params[i].size()[0])\n",
    "wandb.config.policy_nodes = nodes\n",
    "\n",
    "nodes = []\n",
    "params = list(v_net.parameters())\n",
    "for i in range(len(params))[1::2]:\n",
    "    nodes.append(params[i].size()[0])\n",
    "wandb.config.value_nodes = nodes\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    \n",
    "    # tracking\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    old_action_probs = []\n",
    "    \n",
    "    # get start state from env\n",
    "    state = env.reset()\n",
    "    \n",
    "    terminal = False\n",
    "    while terminal is False:\n",
    "        \n",
    "        # choose next action\n",
    "        action, prob = actor(state)\n",
    "        \n",
    "        # take next step and get reward from env\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        # tracking\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        old_action_probs.append(prob.item())\n",
    "        rewards.append(reward*np.power(GAMMA, step)) \n",
    "        \n",
    "        # updates\n",
    "        step += 1\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if step % MINIBATCH == 0 or terminal == True:\n",
    "            \n",
    "            # next state for TD calc\n",
    "            states.append(next_state)\n",
    "            \n",
    "            # reward to go \n",
    "            returns = rewards_to_go(rewards)\n",
    "            \n",
    "            # value estimate\n",
    "            values = get_value(states)\n",
    "        \n",
    "            # advantage estimation using TD error\n",
    "            adv = torch.Tensor([rewards[i] + (1-terminal) * GAMMA * values[i+1].item() - values[i].item() for i in range(len(rewards))])\n",
    "        \n",
    "            # update policy net\n",
    "            optimise_policy(states, actions, old_action_probs, adv)\n",
    "            \n",
    "            # update value net\n",
    "            optimise_v_net(returns, values[:-1])\n",
    "            \n",
    "            if episode != num_episodes-1 or terminal is False:\n",
    "                # clear lists\n",
    "                states = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                old_action_probs = []\n",
    "    \n",
    "\n",
    "    \n",
    "    wandb.log({\"reward\": int(episode_reward)}, step=episode)\n",
    "    episode_rewards.append(int(episode_reward))\n",
    "\n",
    "wandb.config.ave_rewards = np.mean(episode_rewards[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
