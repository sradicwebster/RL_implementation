{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE: Monte Carlo Policy Gradient\n",
    "\n",
    "Involves learning a parameterised policy $\\pi(a|s,\\theta)$ directly. Using gradient ascent, we can move $\\theta$ toward the direction suggested by the gradient of the value function $J(\\theta)$ with respect to $\\theta$ to find the policy that produces the highest return.\n",
    "\n",
    "$$\\theta_{t+1}=\\theta_t+\\nabla_\\theta J(\\theta)$$\n",
    "\n",
    "The Policy Gradient Theorem is an expression for how performance is affected by the policy parameter and does not involve derivatives of state distribution\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)= \\mathop{\\mathbb{E}} [Q^\\pi(s,a) \\nabla_\\theta ln \\space \\pi_\\theta(a|s)]$$\n",
    "\n",
    "REINFORCE relies on an estimated return by Monte-Carlo sampling of full trajectories. The policy update is:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\space \\gamma^t \\space G_t \\space \\nabla_\\theta ln \\space \\pi_\\theta(a|s)$$\n",
    "\n",
    "A widely used variation of REINFORCE is to subtract a baseline value from the return $G_t$ to reduce the variance of gradient estimation while keeping the bias unchanged. A common baseline in the state-value value function $V(s)$.\n",
    "\n",
    "Alternatively, the return in the gradient expression can be replaced with $A(s,a)$, the advantage function (how much more return action $a$ recieves compared to the average action relative to the policy). This has not been implemented here. \n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)= \\mathop{\\mathbb{E}} [A^\\pi(s,a) \\nabla_\\theta ln \\space \\pi_\\theta(a|s)]$$\n",
    "\n",
    "where $A(s,a)=Q(s,a)-V(s)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='reinforce_cartpole')\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU', torch.cuda.is_available())\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Get size of observation space\n",
    "obs_size = env.observation_space.shape[0]\n",
    "print(f'Observation space: {obs_size}')\n",
    "# Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity \n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "print(f'Action space: {n_actions}')\n",
    "# Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_Vfunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Vfunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def softmax_action(state):\n",
    "    policy = policy_net(torch.from_numpy(state).float().to(device))\n",
    "    probs = F.softmax(policy, dim=0)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    action = dist.sample().item()\n",
    "    return action\n",
    "\n",
    "def get_return(rewards):\n",
    "    rewards = torch.Tensor(rewards).to(device)\n",
    "    if reward_to_go is False:\n",
    "        returns = rewards.sum()\n",
    "    elif reward_to_go is True:\n",
    "        returns = torch.stack([rewards[i:].sum() for i in range(len(rewards))]).to(device)\n",
    "    return returns\n",
    "\n",
    "def get_value(states):\n",
    "    return torch.stack([v_net(torch.from_numpy(state).float().to(device))[0] for state in states]).to(device)\n",
    "\n",
    "def optimise_v_net(current_v, returns):\n",
    "    loss_v_net = loss_fn(current_v, returns)\n",
    "    wandb.log({\"value_loss\": loss_v_net}, step=episode)\n",
    "    optimizer_v_net.zero_grad()\n",
    "    loss_v_net.to(device)\n",
    "    loss_v_net.backward()\n",
    "    for param in v_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer_v_net.step()\n",
    "\n",
    "def get_gradient(states, actions, returns, current_v):\n",
    "    log_probs = []\n",
    "    for i, state in enumerate(states):\n",
    "        policy = policy_net(torch.from_numpy(state).float().to(device))\n",
    "        log_probs.append(F.log_softmax(policy, dim=0)[actions[i]])    \n",
    "    log_probs = torch.stack(log_probs).to(device)\n",
    "    phi = returns - current_v\n",
    "    gradient = (log_probs * phi).sum()\n",
    "    return gradient\n",
    "\n",
    "def optimise_policy(loss_policy):\n",
    "    optimizer_policy.zero_grad()\n",
    "    loss_policy.to(device)\n",
    "    loss_policy.backward(retain_graph=True)\n",
    "    #for param in policy_net.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer_policy.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE_POLICY = 5e-4\n",
    "LEARNING_RATE_VALUE = 1e-6\n",
    "reward_to_go = True\n",
    "baseline = False   \n",
    "\n",
    "num_episodes = 2000\n",
    "\n",
    "if reward_to_go == False and baseline == True:\n",
    "    raise ValueError(\"if using episode return then cannot use baseline\")\n",
    "\n",
    "# Save model inputs and hyperparameters\n",
    "wandb.config = wandb.config\n",
    "wandb.config.learning_rate_policy = LEARNING_RATE_POLICY\n",
    "if baseline is True:\n",
    "    wandb.config.learning_rate_value = LEARNING_RATE_VALUE\n",
    "wandb.config.reward_to_go = reward_to_go\n",
    "wandb.config.baseline = baseline\n",
    "\n",
    "# initialise parameterised policy function\n",
    "policy_net = MLP_policy().to(device) \n",
    "optimizer_policy = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE_POLICY)\n",
    "\n",
    "if baseline is True:\n",
    "    v_net = MLP_Vfunction().to(device)\n",
    "    optimizer_v_net = optim.Adam(v_net.parameters(), lr=LEARNING_RATE_VALUE)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "nodes = []\n",
    "params = list(policy_net.parameters())\n",
    "for i in range(len(params))[1::2]:\n",
    "    nodes.append(params[i].size()[0])\n",
    "wandb.config.nn_nodes = nodes\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    \n",
    "    # tracking\n",
    "    step = 0\n",
    "    episode_reward = 0\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    # get start state from env\n",
    "    state = env.reset() \n",
    "    \n",
    "    terminal = False\n",
    "    while terminal is False:\n",
    "        \n",
    "        # choose next action\n",
    "        action = softmax_action(state)\n",
    "        \n",
    "        # take next step and get reward from env\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        # tracking\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward*np.power(GAMMA, step)) \n",
    "        \n",
    "        # updates\n",
    "        step += 1\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    returns = get_return(rewards)\n",
    "    \n",
    "    current_v = 0\n",
    "    if baseline is True:\n",
    "        current_v = get_value(states)\n",
    "\n",
    "    gradient = get_gradient(states, actions, returns, current_v)\n",
    "    optimise_policy(-gradient)\n",
    "    \n",
    "    if baseline is True:\n",
    "        optimise_v_net(current_v, returns)\n",
    "    \n",
    "    wandb.log({\"reward\": episode_reward}, step=episode)\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "wandb.config.ave_rewards = np.mean(episode_rewards[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
