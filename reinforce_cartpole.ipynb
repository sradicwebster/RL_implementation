{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msradicwebster\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.8<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">stilted-plasma-9</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/sradicwebster/reinforce_cartpole\" target=\"_blank\">https://wandb.ai/sradicwebster/reinforce_cartpole</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/sradicwebster/reinforce_cartpole/runs/127j77p4\" target=\"_blank\">https://wandb.ai/sradicwebster/reinforce_cartpole/runs/127j77p4</a><br/>\n",
       "                Run data is saved locally in <code>wandb/run-20201105_181809-127j77p4</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU False\n",
      "Observation space: 4\n",
      "Action space: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='reinforce_cartpole')\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU', torch.cuda.is_available())\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Get size of observation space\n",
    "obs_size = env.observation_space.shape[0]\n",
    "print(f'Observation space: {obs_size}')\n",
    "# Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity \n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "print(f'Action space: {n_actions}')\n",
    "# Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "            \n",
    "    #define the forward function, and the backward function (where gradients are computed)\n",
    "    # is automatically defined for you using autograd\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def forward_prop(network, state):\n",
    "    return network(torch.from_numpy(state).float().to(device))\n",
    "\n",
    "def softmax_action(state):\n",
    "    policy = forward_prop(policy_net, state)\n",
    "    probs = F.softmax(policy, dim=0)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    action = dist.sample().item()\n",
    "    log_prob = F.log_softmax(policy, dim=0)[action]\n",
    "    return action, log_prob\n",
    "\n",
    "def optimise_model(log_probs, discounted_rewards):\n",
    "    returns = [np.sum(discounted_rewards[i:]) for i in range(len(discounted_rewards))]\n",
    "    optimizer.zero_grad()\n",
    "    loss = -torch.stack([returns[i]*log_probs[i] for i in range(len(discounted_rewards))]).sum()\n",
    "    wandb.log({\"loss\": loss.sum()}, step=episode)\n",
    "    #loss.to(gpu)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [24:45<00:00,  6.73it/s]\n"
     ]
    }
   ],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "# Save model inputs and hyperparameters\n",
    "wandb.config = wandb.config\n",
    "wandb.config.learning_rate = LEARNING_RATE\n",
    "\n",
    "# initialise parameterised policy function\n",
    "policy_net = MLP().to(device) \n",
    "\n",
    "nodes = []\n",
    "params = list(policy_net.parameters())\n",
    "for i in range(len(params))[1::2]:\n",
    "    nodes.append(params[i].size()[0])\n",
    "wandb.config.nn_nodes = nodes\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    \n",
    "    # reset step count\n",
    "    step = 0\n",
    "    cum_discounted_reward = 0\n",
    "    episode_reward = 0\n",
    "    discounted_rewards = []\n",
    "    log_probs = []\n",
    "    \n",
    "    # get start state from env\n",
    "    state = env.reset() \n",
    "    \n",
    "    terminal = False\n",
    "    while terminal is False:\n",
    "        \n",
    "        # choose next action\n",
    "        action, log_prob = softmax_action(state)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # take next step and get reward from env\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        # reward tracking\n",
    "        #cum_discounted_reward += reward * np.power(GAMMA, step)\n",
    "        discounted_rewards.append(reward*np.power(GAMMA, step)) \n",
    "        \n",
    "        # updates\n",
    "        state = next_state\n",
    "        step += 1\n",
    "        episode_reward += reward\n",
    "       \n",
    "\n",
    "    episode_rewards.append(episode_reward)\n",
    "    wandb.log({\"reward\": episode_reward}, step=episode)\n",
    "            \n",
    "    optimise_model(log_probs, discounted_rewards)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
