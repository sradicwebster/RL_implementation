{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic\n",
    "\n",
    "The policy update based on the policy gradient theorem is \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\space \\gamma^t \\space Q(s,a) \\space \\nabla_\\theta ln \\space \\pi_\\theta(a|s)$$\n",
    "\n",
    "In REINFORCE Monte Carlo sampling of episode return was used to estimate $Q(s,a)$ while a parameterised state-value function $V(s)$ was learnt as a baseline to reduce the variance of the gradient estimation (while keeping the bias unchanged). \n",
    "\n",
    "In actor critic methods a $Q(s,a)$ is directly learnt. Again $V(s)$ can be subtracted from $Q(s,a)$ which is known as the advantage $A(s,a)$ (how much more return action $a$ recieves compared to the average action relative to the policy). The TD error can be used as an estimate of the advantage function.\n",
    "\n",
    "$$A(s,a) = Q(s,a)-V(s)=R + \\gamma \\space V(s^\\prime) - V(s) $$\n",
    "\n",
    "The policy update takes a step proportional to the one step TD error, calculated using a parameterised value function (learnt by regression on the mean-square TD error):\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\space \\gamma^t \\space \\delta \\space \\nabla_\\theta ln \\space \\pi_\\theta(a|s)$$\n",
    "\n",
    "where: $\\delta = R + \\gamma \\space V(s^\\prime) - V(s) $\n",
    "\n",
    "The value function assigns credit to the policy’s action selections – critic the actor.\n",
    "\n",
    "Bias is introduced through bootstrapping (updating the value estimate for a state from the estimated values of subsequent states) which reduces variance and speeds up learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='a2C_cartpole')\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('GPU', torch.cuda.is_available())\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Get size of observation space\n",
    "obs_size = env.observation_space.shape[0]\n",
    "print(f'Observation space: {obs_size}')\n",
    "# Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity \n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "print(f'Action space: {n_actions}')\n",
    "# Left, Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class MLP_Vfunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Vfunction, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, 64) \n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def softmax_action(state):\n",
    "    policy = policy_net(torch.from_numpy(state).float().to(device))\n",
    "    probs = F.softmax(policy, dim=0)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    action = dist.sample().item()\n",
    "    return action\n",
    "\n",
    "def get_value(state):\n",
    "    return v_net(torch.from_numpy(state).float().to(device))\n",
    "\n",
    "def optimise_v_net(target, current_v):\n",
    "    loss_v_net = loss_fn(target, current_v)\n",
    "    wandb.log({\"value_loss\": loss_v_net}, step=episode)\n",
    "    optimizer_v_net.zero_grad()\n",
    "    loss_v_net.to(device)\n",
    "    loss_v_net.backward()#retain_graph=True)\n",
    "    #for param in v_net.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer_v_net.step()\n",
    "\n",
    "def get_lob_prob(state, action):\n",
    "    policy = policy_net(torch.from_numpy(state).float().to(device))\n",
    "    log_prob = F.log_softmax(policy, dim=0)[action]    \n",
    "    return log_prob\n",
    "\n",
    "def optimise_policy(error, I, state, action):\n",
    "    log_prob = get_lob_prob(state, action)\n",
    "    loss_policy = -I * error * log_prob\n",
    "    optimizer_policy.zero_grad()\n",
    "    loss_policy.to(device)\n",
    "    loss_policy.backward()#retain_graph=True)\n",
    "    #for param in policy_net.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer_policy.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE_POLICY = 1e-4\n",
    "LEARNING_RATE_VALUE = 5e-3 # want critic to update faster\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "# Save model inputs and hyperparameters\n",
    "wandb.config = wandb.config\n",
    "wandb.config.learning_rate_policy = LEARNING_RATE_POLICY\n",
    "wandb.config.learning_rate_value = LEARNING_RATE_VALUE\n",
    "\n",
    "# initialise parameterised policy function\n",
    "policy_net = MLP_policy().to(device) \n",
    "optimizer_policy = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE_POLICY)\n",
    "\n",
    "v_net = MLP_Vfunction().to(device)\n",
    "optimizer_v_net = optim.Adam(v_net.parameters(), lr=LEARNING_RATE_VALUE)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "nodes = []\n",
    "params = list(policy_net.parameters())\n",
    "for i in range(len(params))[1::2]:\n",
    "    nodes.append(params[i].size()[0])\n",
    "wandb.config.nn_nodes = nodes\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    \n",
    "    # tracking\n",
    "    I = 1\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # get start state from env\n",
    "    state = env.reset() \n",
    "    \n",
    "    terminal = False\n",
    "    while terminal is False:\n",
    "        \n",
    "        # choose next action\n",
    "        action = softmax_action(state)\n",
    "        \n",
    "        # take next step and get reward from env\n",
    "        next_state, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        # TD error\n",
    "        target = reward + (1-terminal) * GAMMA * get_value(next_state)\n",
    "        current_v = get_value(state)\n",
    "        error = (target - current_v).item()\n",
    "        \n",
    "        # update value net\n",
    "        optimise_v_net(target, current_v)\n",
    "        \n",
    "        # update policy net\n",
    "        optimise_policy(error, I, state, action)\n",
    "        \n",
    "        # updates\n",
    "        I *= GAMMA\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    wandb.log({\"reward\": episode_reward}, step=episode)\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "wandb.config.ave_rewards = np.mean(episode_rewards[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
